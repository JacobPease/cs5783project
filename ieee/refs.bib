@InProceedings{model-soups,
    title = 	 {Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
    author =       {Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig},
    booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
    pages = 	 {23965--23998},
    year = 	 {2022},
    editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
    volume = 	 {162},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {17--23 Jul},
    publisher =    {PMLR},
    pdf = 	 {https://proceedings.mlr.press/v162/wortsman22a/wortsman22a.pdf},
    url = 	 {https://proceedings.mlr.press/v162/wortsman22a.html}
}

@misc{ldm,
    doi = {10.48550/ARXIV.2204.11824},
    url = {https://arxiv.org/abs/2204.11824},
    author = {Blattmann, Andreas and Rombach, Robin and Oktay, Kaan and Ommer, Björn},
    keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Retrieval-Augmented Diffusion Models},
    publisher = {arXiv},
    year = {2022},  
    copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{esser2024scaling,
    title={Scaling Rectified Flow Transformers for High-Resolution Image Synthesis},
    author={Patrick Esser and Sumith Kulal and Andreas Blattmann and Rahim Entezari and Jonas M{\"u}ller and Harry Saini and Yam Levi and Dominik Lorenz and Axel Sauer and Frederic Boesel and Dustin Podell and Tim Dockhorn and Zion English and Robin Rombach},
    booktitle={Forty-first International Conference on Machine Learning},
    year={2024},
    url={https://openreview.net/forum?id=FPnUhsQJ5B}
}

@inproceedings{ho2020denoising,
    author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
    pages = {6840--6851},
    publisher = {Curran Associates, Inc.},
    title = {Denoising Diffusion Probabilistic Models},
    url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
    volume = {33},
    year = {2020}
}

@article{brock2018large,
    author       = {Andrew Brock and
                  Jeff Donahue and
                  Karen Simonyan},
    title        = {Large Scale {GAN} Training for High Fidelity Natural Image Synthesis},
    journal      = {CoRR},
    volume       = {abs/1809.11096},
    year         = {2018},
    url          = {http://arxiv.org/abs/1809.11096},
    eprinttype    = {arXiv},
    eprint       = {1809.11096},
    timestamp    = {Fri, 05 Oct 2018 11:34:52 +0200},
    biburl       = {https://dblp.org/rec/journals/corr/abs-1809-11096.bib},
    bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{child2020very,
    author       = {Rewon Child},
    title        = {Very Deep VAEs Generalize Autoregressive Models and Can Outperform
                  Them on Images},
    journal      = {CoRR},
    volume       = {abs/2011.10650},
    year         = {2020},
    url          = {https://arxiv.org/abs/2011.10650},
    eprinttype    = {arXiv},
    eprint       = {2011.10650},
    timestamp    = {Wed, 25 Nov 2020 16:34:14 +0100},
    biburl       = {https://dblp.org/rec/journals/corr/abs-2011-10650.bib},
    bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{chen2020generative,
    title = 	 {Generative Pretraining From Pixels},
    author =       {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
    booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
    pages = 	 {1691--1703},
    year = 	 {2020},
    editor = 	 {III, Hal Daumé and Singh, Aarti},
    volume = 	 {119},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {13--18 Jul},
    publisher =    {PMLR},
    pdf = 	 {http://proceedings.mlr.press/v119/chen20s/chen20s.pdf},
    url = 	 {https://proceedings.mlr.press/v119/chen20s.html},
    abstract = 	 {Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full fine-tuning, matching the top supervised pre-trained models. We are also competitive with self-supervised benchmarks on ImageNet when substituting pixels for a VQVAE encoding, achieving 69.0% top-1 accuracy on a linear probe of our features.}
}
